{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import download\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking oauth\n",
      "  Downloading oauth-1.0.1.tar.gz\n",
      "  Running setup.py (path:/tmp/pip_build_root/oauth/setup.py) egg_info for package oauth\n",
      "    \n",
      "Downloading/unpacking oauth2\n",
      "  Downloading oauth2-1.9.0.post1-py2.py3-none-any.whl\n",
      "Downloading/unpacking httplib2 (from oauth2)\n",
      "  Downloading httplib2-0.9.2.zip (210kB): 210kB downloaded\n",
      "  Running setup.py (path:/tmp/pip_build_root/httplib2/setup.py) egg_info for package httplib2\n",
      "    \n",
      "Installing collected packages: oauth, oauth2, httplib2\n",
      "  Running setup.py install for oauth\n",
      "    \n",
      "  Running setup.py install for httplib2\n",
      "    \n",
      "Successfully installed oauth oauth2 httplib2\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install oauth oauth2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(tweet):\n",
    "    # removes links, usernames, twitter special word 'RT' (retweet) and emoticons\n",
    "    p = re.compile('(http[s]*://[^\\s]+|@[^\\s]*|RT|(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$))|&\\w*')\n",
    "    return p.sub(\"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanitize(text):\n",
    "    # removes punctuation character\n",
    "    return text.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(tokens):\n",
    "    return [token for token in tokens if  token not in [\"an\", \"a\", \"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n=2): \n",
    "  return zip(*[input_list[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chain2(nested_list):\n",
    "    for l in neg_clean:\n",
    "        for tup in l:\n",
    "            yield(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotTagDistrubition(pos_dist):\n",
    "    N = len(pos_dist)\n",
    "    neg_counts = pos_dist.values()\n",
    "\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.35       # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(ind, neg_counts, width, color='r')\n",
    "    fig.set_size_inches(25,8)\n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(r'P^T values')\n",
    "    ax.set_xticks(ind + width)\n",
    "    ax.set_xticklabels(pos_dist.keys())\n",
    "\n",
    "    #ax.legend((rects1[0], rects2[0]), ('Men', 'Women'))\n",
    "    ax.legend((rects1[0], ('Negative')))\n",
    "\n",
    "    autolabel(rects1)\n",
    "    #autolabel(rects2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTokenizedTweets(rawTweets):\n",
    "    clean = [removeStopWords(word_tokenize(process(tweet[\"text\"]))) for tweet in json.loads(rawTweets[0])[\"statuses\"]]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPosTweetsFromTokenized(tokenizedTweets):\n",
    "    return [pos_tag(tweet) for tweet in getTokenizedTweets(neu_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nl():\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ds/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ds/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"averaged_perceptron_tagger\")\n",
    "download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching some tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_tweets = !python ./twitterstream.py \"https://api.twitter.com/1.1/search/tweets.json?q=%3A%28&lang=en&count=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_id = json.loads(neg_tweets[0])[\"search_metadata\"][\"max_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_tweets = !python ./twitterstream.py \"https://api.twitter.com/1.1/search/tweets.json?q=%3A%28&lang=en&count=100\"\n",
    "negs = negs + getTokenizedTweets(neg_tweets)\n",
    "max_id = json.loads(neg_tweets[0])[\"search_metadata\"][\"max_id\"]\n",
    "for i in range(2):\n",
    "    neg_tweets = !python ./twitterstream.py \"https://api.twitter.com/1.1/search/tweets.json?q=%3A%28&lang=en&count=100&max_id=\"+max_id\n",
    "    negs = negs + getTokenizedTweets(neg_tweets)\n",
    "    max_id = json.loads(neg_tweets[0])[\"search_metadata\"][\"max_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = !python ./twitterstream.py \"https://api.twitter.com/1.1/search/tweets.json?q=%3A%29&lang=eu&count=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neu_tweets = !python ./twitterstream.py \"https://api.twitter.com/1.1/search/tweets.json?q=from%3AHuffingtonPost&count=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative tweets dataset size: 100\n",
      "Positive tweets dataset size: 100\n",
      "Neutral tweets dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "print (\"Negative tweets dataset size: \" + str(len(getTokenizedTweets(neg_tweets))))\n",
    "print (\"Positive tweets dataset size: \" + str(len(getTokenizedTweets(pos_tweets))))\n",
    "print (\"Neutral tweets dataset size: \" + str(len(getTokenizedTweets(neu_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"./tweets_20160804_1816.csv\", nrows=10000)\n",
    "#tweets = getTokenizedTweets(neu_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>So scared to apply for FSU cause it's so far :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>negative</td>\n",
       "      <td>Devastated that my girlfriends cats gone :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>negative</td>\n",
       "      <td>@exeliax yeah, please lie in a bed and rest :(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Spratt Fast pace nuclear are great to watch. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>negative</td>\n",
       "      <td>@BlizzardCS Hello! I am playing the new @StarC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>negative</td>\n",
       "      <td>@inkedIwt @littleIouie would too :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>negative</td>\n",
       "      <td>Feels amazing walking around with 100,000 lira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>negative</td>\n",
       "      <td>@MangoIts i feel you on that ive been averagin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>negative</td>\n",
       "      <td>Literally going to my apartment just to sleep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Rumil_DB i need more melee :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Ashriel_21 im not working today :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>negative</td>\n",
       "      <td>how i forgot meiko i'll never know. cya fred :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>negative</td>\n",
       "      <td>i had a dream that i had all these baggies of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>negative</td>\n",
       "      <td>Why is life so hard!? :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>negative</td>\n",
       "      <td>I'm almost hitting mc every month. Is it norma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>negative</td>\n",
       "      <td>@OhBrokenMsgs the story of my life :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>negative</td>\n",
       "      <td>@icons_8 for android alone we need them in a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>negative</td>\n",
       "      <td>@BigMike7750 @SixxsRockAngel @dallascowboys @n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>negative</td>\n",
       "      <td>My face is getting chubbier nd my sis said \"u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>negative</td>\n",
       "      <td>I don't know you love :(  https://t.co/wX2HYwwlIh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>negative</td>\n",
       "      <td>Done with job number 1, got another hour to bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>negative</td>\n",
       "      <td>I just want to be free from anxiety I can't ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>negative</td>\n",
       "      <td>@BethRice_ way way way way too long! I miss ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>negative</td>\n",
       "      <td>My son also upset  https://t.co/sR7bMwRJcQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>negative</td>\n",
       "      <td>@RevAGSL I haven't gotten anything :( I'll DM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>negative</td>\n",
       "      <td>she mean :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Courtney94x The show does love kicking her ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>negative</td>\n",
       "      <td>@alekaaa_ @_becex hi guys can we all hang out :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>negative</td>\n",
       "      <td>help, where's a cheap place i can get my phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>negative</td>\n",
       "      <td>@davidrosam You seem to have plenty of good co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>negative</td>\n",
       "      <td>@JayShams very true :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>negative</td>\n",
       "      <td>@FaZeRug im sorry that youre having to go thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>negative</td>\n",
       "      <td>@alexct9a I am a man lol :( old gang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>negative</td>\n",
       "      <td>@mutekitten i'm always here for you love !! :(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>negative</td>\n",
       "      <td>I've been searching for the cherry flavor all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>negative</td>\n",
       "      <td>the liveshow is so laggy today :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Looserlexyy I haven't been able to watch it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>negative</td>\n",
       "      <td>@gxsellesalazar everybody's moving now :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>negative</td>\n",
       "      <td>idk what to do with this mess :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>negative</td>\n",
       "      <td>Why do I always get so hungry when I'm broke :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>negative</td>\n",
       "      <td>@_RunningHappy can tell the dark nights are st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9928</th>\n",
       "      <td>negative</td>\n",
       "      <td>greaaatt I love bein sick :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>negative</td>\n",
       "      <td>I be sad when I'm having a good convo with som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>negative</td>\n",
       "      <td>@bowlingotter @LissySandwich Have to out my ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9944</th>\n",
       "      <td>negative</td>\n",
       "      <td>Oh, all these #AGU abstracts here.. Would love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9945</th>\n",
       "      <td>negative</td>\n",
       "      <td>@pashbarak Maybe we'll pull an APOEL Nicosia a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946</th>\n",
       "      <td>negative</td>\n",
       "      <td>Boo I wanna set up Charlotte and download the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>negative</td>\n",
       "      <td>@liljipsi you're so lucky tf :( and sorry for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9957</th>\n",
       "      <td>negative</td>\n",
       "      <td>Today it would be 27 years old Jules :( . We m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9960</th>\n",
       "      <td>negative</td>\n",
       "      <td>@iheartyoson @JeongHyongkimVx but he needs hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>negative</td>\n",
       "      <td>No we don't sell yeezys :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9966</th>\n",
       "      <td>negative</td>\n",
       "      <td>@danalavecc pack me :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>negative</td>\n",
       "      <td>I can't even swim :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>negative</td>\n",
       "      <td>@urbandoII I'm sorry :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>negative</td>\n",
       "      <td>@PokemonGoApp Pokemon Club not works :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>negative</td>\n",
       "      <td>Bordelon possibly more enthusiastic abt @socia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>negative</td>\n",
       "      <td>The WORLD wake up - give young children guns t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>negative</td>\n",
       "      <td>@_kallyy @Eliijxhhh where am I :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>negative</td>\n",
       "      <td>why isn't my Instagram++ not working? the twea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>negative</td>\n",
       "      <td>Adulting took longer than I expected today, go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2380 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tag                                               text\n",
       "7     negative    So scared to apply for FSU cause it's so far :(\n",
       "10    negative        Devastated that my girlfriends cats gone :(\n",
       "11    negative  @exeliax yeah, please lie in a bed and rest :(...\n",
       "15    negative  @Spratt Fast pace nuclear are great to watch. ...\n",
       "28    negative  @BlizzardCS Hello! I am playing the new @StarC...\n",
       "30    negative                @inkedIwt @littleIouie would too :(\n",
       "33    negative  Feels amazing walking around with 100,000 lira...\n",
       "53    negative  @MangoIts i feel you on that ive been averagin...\n",
       "55    negative  Literally going to my apartment just to sleep ...\n",
       "57    negative                     @Rumil_DB i need more melee :(\n",
       "59    negative                @Ashriel_21 im not working today :(\n",
       "68    negative    how i forgot meiko i'll never know. cya fred :(\n",
       "69    negative  i had a dream that i had all these baggies of ...\n",
       "78    negative                           Why is life so hard!? :(\n",
       "79    negative  I'm almost hitting mc every month. Is it norma...\n",
       "87    negative              @OhBrokenMsgs the story of my life :(\n",
       "92    negative  @icons_8 for android alone we need them in a r...\n",
       "93    negative  @BigMike7750 @SixxsRockAngel @dallascowboys @n...\n",
       "95    negative  My face is getting chubbier nd my sis said \"u ...\n",
       "106   negative  I don't know you love :(  https://t.co/wX2HYwwlIh\n",
       "114   negative  Done with job number 1, got another hour to bu...\n",
       "121   negative  I just want to be free from anxiety I can't ta...\n",
       "124   negative  @BethRice_ way way way way too long! I miss ou...\n",
       "126   negative         My son also upset  https://t.co/sR7bMwRJcQ\n",
       "127   negative  @RevAGSL I haven't gotten anything :( I'll DM ...\n",
       "135   negative                                        she mean :(\n",
       "139   negative  @Courtney94x The show does love kicking her ar...\n",
       "142   negative   @alekaaa_ @_becex hi guys can we all hang out :(\n",
       "149   negative  help, where's a cheap place i can get my phone...\n",
       "160   negative  @davidrosam You seem to have plenty of good co...\n",
       "...        ...                                                ...\n",
       "9876  negative                             @JayShams very true :(\n",
       "9879  negative  @FaZeRug im sorry that youre having to go thro...\n",
       "9887  negative               @alexct9a I am a man lol :( old gang\n",
       "9891  negative  @mutekitten i'm always here for you love !! :(...\n",
       "9895  negative  I've been searching for the cherry flavor all ...\n",
       "9901  negative                  the liveshow is so laggy today :(\n",
       "9904  negative  @Looserlexyy I haven't been able to watch it t...\n",
       "9910  negative          @gxsellesalazar everybody's moving now :(\n",
       "9915  negative                   idk what to do with this mess :(\n",
       "9916  negative    Why do I always get so hungry when I'm broke :(\n",
       "9919  negative  @_RunningHappy can tell the dark nights are st...\n",
       "9928  negative                       greaaatt I love bein sick :(\n",
       "9932  negative  I be sad when I'm having a good convo with som...\n",
       "9939  negative  @bowlingotter @LissySandwich Have to out my ca...\n",
       "9944  negative  Oh, all these #AGU abstracts here.. Would love...\n",
       "9945  negative  @pashbarak Maybe we'll pull an APOEL Nicosia a...\n",
       "9946  negative  Boo I wanna set up Charlotte and download the ...\n",
       "9951  negative  @liljipsi you're so lucky tf :( and sorry for ...\n",
       "9957  negative  Today it would be 27 years old Jules :( . We m...\n",
       "9960  negative  @iheartyoson @JeongHyongkimVx but he needs hel...\n",
       "9961  negative                         No we don't sell yeezys :(\n",
       "9966  negative                             @danalavecc pack me :(\n",
       "9973  negative                               I can't even swim :(\n",
       "9974  negative                            @urbandoII I'm sorry :(\n",
       "9978  negative            @PokemonGoApp Pokemon Club not works :(\n",
       "9987  negative  Bordelon possibly more enthusiastic abt @socia...\n",
       "9989  negative  The WORLD wake up - give young children guns t...\n",
       "9993  negative                  @_kallyy @Eliijxhhh where am I :(\n",
       "9998  negative  why isn't my Instagram++ not working? the twea...\n",
       "9999  negative  Adulting took longer than I expected today, go...\n",
       "\n",
       "[2380 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets[\"tag\"] == \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    processed = process(tweet.decode(\"utf-8\"))\n",
    "    return [w.lower() for w in removeStopWords(word_tokenize(processed)) if w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TEXT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0291d64c7004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CLEAN\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TEXT\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1967\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1974\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1975\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1976\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1978\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3211\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3212\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3213\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1757\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m   1758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TEXT'"
     ]
    }
   ],
   "source": [
    "tweets[\"CLEAN\"] = tweets[\"TEXT\"].map(clean)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CLEAN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-86686cd32eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CLEAN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1802\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CLEAN'"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for index, tweet in tweets.iterrows():\n",
    "    all_words.extend(tweet[\"CLEAN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131624"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = get_word_features(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in word_features:\n",
    "    tweets[word] = tweets.CLEAN.apply(lambda x: word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = tweets.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = tweets[word_features]\n",
    "y = tweets[\"TAG\"]\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(test_data[word_features])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89000000000000001"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = predictions == test_data[\"TAG\"] \n",
    "sum(precision)/float(len(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93430000000000002"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tweets[\"TAG\"] == \"positive\") / float(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = [\"positive\" == k for k in test_data[\"TAG\"] ]\n",
    "sum(precision)/float(len(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We remove URL links, Twitter user names and Twitter special words (e.g. \"RT\" (retweet)) and emoticons   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: And now for something !!don't I'm completely different the http://google.fr\n",
      "After: And now for something !!don't I'm completely different the \n"
     ]
    }
   ],
   "source": [
    "processed = process(tweet)\n",
    "print (\"Before: \" + tweet)\n",
    "print (\"After: \" + processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: And now for something !!don't I'm completely different the http://google.fr\n",
      "After: ['And', 'now', 'for', 'something', '!', '!', 'do', \"n't\", 'I', \"'m\", 'completely', 'different', 'the']\n"
     ]
    }
   ],
   "source": [
    "tokenized = word_tokenize(processed)\n",
    "print (\"Before: \" + tweet)\n",
    "print (\"After: \" + str(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words from bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: And now for something !!don't I'm completely different the http://google.fr\n",
      "After: ['And', 'now', 'for', 'something', '!', '!', 'do', \"n't\", 'I', \"'m\", 'completely', 'different']\n"
     ]
    }
   ],
   "source": [
    "withoutStopWords = removeStopWords(tokenized)\n",
    "print (\"Before: \" + tweet)\n",
    "print (\"After: \" + str(withoutStopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Part-Of-Speech tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: And now for something !!don't I'm completely different the http://google.fr\n",
      "After: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('!', '.'), ('!', '.'), ('do', 'VBP'), (\"n't\", 'RB'), ('I', 'PRP'), (\"'m\", 'VBP'), ('completely', 'RB'), ('different', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "pos = pos_tag(withoutStopWords)\n",
    "print (\"Before: \" + tweet)\n",
    "print (\"After: \" + str(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features set 1: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('do', 'VBP'), (\"n't\", 'RB'), ('I', 'PRP'), (\"'m\", 'VBP'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "\n",
      "\n",
      "Features set 2: [('And', 'now'), ('now', 'for'), ('for', 'something'), ('something', 'do'), ('do', \"n't\"), (\"n't\", 'I'), ('I', \"'m\"), (\"'m\", 'completely'), ('completely', 'different')]\n"
     ]
    }
   ],
   "source": [
    "features_pos = [(k,v) for (k,v) in pos if k not in  string.punctuation ]\n",
    "features_ngrams = find_ngrams([k for (k,_) in features_pos])\n",
    "print (\"Features set 1: \" + str(features_pos))\n",
    "nl()\n",
    "print (\"Features set 2: \" + str(features_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature engineering (Please bring some coffee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neutral_pos = getPosTweetsFromTokenized(getTokenizedTweets(neu_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_pos = [(k,v) for (k,v) in chain2(neutral_pos) if k not in  string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neu_pos = [(k,v) for (k,v) in chain2(neutral_pos) if k not in  string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neu_pos_dist = collections.Counter([pos for (_,pos) in neu_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotTagDistrubition(neu_pos_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
